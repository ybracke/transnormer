# -*- coding = utf-8 -*-

# Select GPU
gpu = "cuda:0"

# Fix random seed for reproducibility
random_seed = 42

[data]
path_test = "data/dtak-transnormer-basic-v1/data/test/1700-1799"
# split = "test"
# n_examples_test = 100
max_bytelength = 512

[tokenizer]
checkpoint_in = "google/byt5-small"

[tokenizer_configs]
padding = "longest" # pad to the longest sequence in batch
truncation = false

[model]
checkpoint = "ybracke/transnormer-18-19c-beta-v01"

[generation]
batch_size = 32

[generation_config]
# See here for more options: https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig
max_new_tokens = 600
early_stopping = false
num_beams = 4
